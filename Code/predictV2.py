import torch
import cv2
import numpy as np
import os
import time
from tqdm import tqdm

# --- 1. C·∫§U H√åNH (CONFIGURATION) ---
MODEL_PATH = r"E:\DSUnet-drivable_area_segmentation\Experiments\DSUnet_MultiClass_Fixed_20251218_150629\models\dsunet_deploy_miou_0.679.pt"
INPUT_PATH = r"E:\DSUnet-drivable_area_segmentation\Data\Video\mixkit-point-of-view-from-a-bus-passenger-seat-roading-in-4394-hd-ready.mp4" 
OUTPUT_DIR = "inference_results_optimized"

# C·∫•u h√¨nh x·ª≠ l√Ω
MIN_LANE_AREA = 100     # Di·ªán t√≠ch t·ªëi thi·ªÉu (pixel) tr√™n ·∫£nh nh·ªè ƒë·ªÉ ch·∫•p nh·∫≠n l√† 1 l√†n ƒë∆∞·ªùng
SAMPLE_STEP = 10        # B∆∞·ªõc nh·∫£y khi qu√©t h√†ng (tr√™n ·∫£nh nh·ªè)

# M√†u s·∫Øc
OVERLAY_COLORS = {
    0: [0, 0, 0],
    1: [0, 255, 0],     # Lane A (Green)
    2: [0, 0, 255],     # Lane B (Red)
}

PATH_COLORS = {
    0: [0, 0, 0],
    1: [255, 0, 255],   # Path A (Magenta)
    2: [0, 255, 255],   # Path B (Yellow)
}

ALPHA = 0.5

# --- 2. CLASS D·ª∞ ƒêO√ÅN ---
class LanePredictor:
    def __init__(self, model_path, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        print(f"üîÑ Loading model: {model_path}")
        print(f"‚öô  Device: {self.device}")
        try:
            self.model = torch.jit.load(model_path, map_location=self.device)
            self.model.eval()
            # Warm-up
            dummy = torch.zeros((1, 288, 512, 3), dtype=torch.uint8).to(self.device)
            with torch.no_grad(): self.model(dummy)
            print("‚úÖ Model Ready!")
        except Exception as e:
            print(f"‚ùå Error: {e}")
            exit()

    def predict_raw(self, img_bgr):
        """
        Tr·∫£ v·ªÅ mask th√¥ k√≠ch th∆∞·ªõc nh·ªè (theo output model) ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô x·ª≠ l√Ω.
        """
        img_tensor = torch.from_numpy(img_bgr).unsqueeze(0).to(self.device)
        with torch.no_grad():
            probs = self.model(img_tensor)
        # Output: [1, C, H, W] -> [H, W]
        pred_mask = torch.argmax(probs, dim=1).squeeze().cpu().numpy().astype(np.uint8)
        return pred_mask

# --- 3. X·ª¨ L√ù PATH (OPTIMIZED) ---
def get_separated_paths(binary_mask, cls_id, scale_x, scale_y):
    """
    1. T√°ch c√°c l√†n ƒë∆∞·ªùng ri√™ng bi·ªát (Connected Components).
    2. T√≠nh Cubic Path cho t·ª´ng l√†n.
    3. Scale t·ªça ƒë·ªô v·ªÅ k√≠ch th∆∞·ªõc video g·ªëc.
    """
    # B∆∞·ªõc 1: Ti·ªÅn x·ª≠ l√Ω - Lo·∫°i b·ªè nhi·ªÖu (Outlier Removal)
    # D√πng ph√©p m·ªü (Opening) ƒë·ªÉ x√≥a c√°c ƒëi·ªÉm nhi·ªÖu l·ªëm ƒë·ªëm
    kernel = np.ones((3,3), np.uint8)
    clean_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel)

    # B∆∞·ªõc 2: T√°ch c√°c ƒë·ªëi t∆∞·ª£ng (Instance Separation)
    # ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng ƒë·ªÉ kh√¥ng n·ªëi 2 l√†n ƒë∆∞·ªùng xa nhau
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(clean_mask, connectivity=8)
    
    paths = []
    
    # Duy·ªát qua t·ª´ng "h√≤n ƒë·∫£o" (blob) t√¨m th·∫•y (b·ªè qua label 0 l√† n·ªÅn ƒëen)
    for i in range(1, num_labels):
        area = stats[i, cv2.CC_STAT_AREA]
        
        # B·ªè qua n·∫øu v√πng qu√° nh·ªè (nhi·ªÖu)
        if area < MIN_LANE_AREA: 
            continue
            
        # L·∫•y bounding box ƒë·ªÉ qu√©t cho nhanh (Optimization)
        x_box, y_box, w_box, h_box = stats[i, cv2.CC_STAT_LEFT], stats[i, cv2.CC_STAT_TOP], stats[i, cv2.CC_STAT_WIDTH], stats[i, cv2.CC_STAT_HEIGHT]
        
        # B∆∞·ªõc 3: Sampling (L·∫•y m·∫´u ƒëi·ªÉm)
        sample_x = []
        sample_y = []
        
        # Ch·ªâ qu√©t trong v√πng bounding box c·ªßa blob n√†y
        # Qu√©t t·ª´ d∆∞·ªõi l√™n tr√™n
        for r in range(y_box + h_box - 1, y_box, -SAMPLE_STEP):
            # C·∫Øt m·ªôt l√°t ngang trong v√πng labels
            row_slice = labels[r, x_box : x_box + w_box]
            
            # T√¨m c√°c pixel thu·ªôc v·ªÅ blob th·ª© i
            indices = np.where(row_slice == i)[0]
            
            if len(indices) > 0:
                # T√≠nh t√¢m t∆∞∆°ng ƒë·ªëi + offset x_box
                center_x = x_box + int((indices[0] + indices[-1]) / 2)
                sample_x.append(center_x)
                sample_y.append(r)
        
        # C·∫ßn √≠t nh·∫•t 4 ƒëi·ªÉm ƒë·ªÉ fit b·∫≠c 3
        if len(sample_y) < 4: 
            continue

        try:
            # B∆∞·ªõc 4: Fit ƒêa th·ª©c B·∫≠c 3
            fit_params = np.polyfit(sample_y, sample_x, 3)
            poly_func = np.poly1d(fit_params)
            
            # T·∫°o ƒëi·ªÉm v·∫Ω tr∆°n tru
            plot_y = np.linspace(min(sample_y), max(sample_y), num=50)
            plot_x = poly_func(plot_y)
            
            # B∆∞·ªõc 5: Scale to·∫° ƒë·ªô v·ªÅ video g·ªëc (Upscaling)
            plot_x_scaled = plot_x * scale_x
            plot_y_scaled = plot_y * scale_y
            
            # Gom l·∫°i th√†nh format polylines
            pts = np.array([np.transpose(np.vstack([plot_x_scaled, plot_y_scaled]))], np.int32)
            paths.append(pts)
            
        except:
            continue
            
    return paths

# --- 4. VISUALIZATION ---
def draw_results(frame, mask_small, paths_dict):
    """
    V·∫Ω overlay v√† path l√™n khung h√¨nh g·ªëc.
    """
    h_orig, w_orig = frame.shape[:2]
    
    # 1. V·∫Ω Overlay (Resize mask nh·ªè -> to)
    # D√πng Nearest ƒë·ªÉ gi·ªØ nguy√™n gi√° tr·ªã class
    mask_large = cv2.resize(mask_small, (w_orig, h_orig), interpolation=cv2.INTER_NEAREST)
    
    color_mask = np.zeros_like(frame)
    unique_ids = np.unique(mask_small) # Check tr√™n mask nh·ªè cho nhanh
    
    for cls_id in unique_ids:
        if cls_id == 0: continue
        if cls_id in OVERLAY_COLORS:
            color_mask[mask_large == cls_id] = OVERLAY_COLORS[cls_id]
            
    # Blend Overlay
    mask_bool = (mask_large > 0)
    if np.any(mask_bool):
        # Ch·ªâ blend v√πng c·∫ßn thi·∫øt ƒë·ªÉ tƒÉng t·ªëc
        frame[mask_bool] = cv2.addWeighted(frame[mask_bool], 1-ALPHA, color_mask[mask_bool], ALPHA, 0)
        
    # 2. V·∫Ω Path (ƒê√£ ƒë∆∞·ª£c t√≠nh to√°n ri√™ng bi·ªát)
    for cls_id, paths in paths_dict.items():
        color = PATH_COLORS.get(cls_id, [255, 255, 255])
        for line_pts in paths:
            cv2.polylines(frame, line_pts, isClosed=False, color=color, thickness=4)
            
    return frame

# --- 5. MAIN PROCESS ---
def process_video(predictor, video_path):
    if not os.path.exists(video_path):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y video: {video_path}")
        return

    cap = cv2.VideoCapture(video_path)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps_input = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    save_path = os.path.join(OUTPUT_DIR, f"optimized_{os.path.basename(video_path)}")
    out = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps_input, (width, height))
    
    print(f"üé¨ Processing: {width}x{height} @ {fps_input}fps")
    
    # Pre-calculate scale factors
    # Model output size is usually 512x288 based on your previous code
    # If your model output is different, change these values
    MODEL_W, MODEL_H = 512, 288 
    scale_x = width / MODEL_W
    scale_y = height / MODEL_H
    
    pbar = tqdm(total=total_frames)
    prev_time = time.time()
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break
        
        # 1. Resize Input cho Model (TƒÉng t·ªëc Inference)
        img_small = cv2.resize(frame, (MODEL_W, MODEL_H))
        
        # 2. Predict (Tr·∫£ v·ªÅ mask nh·ªè)
        mask_small = predictor.predict_raw(img_small)
        
        # 3. T√≠nh to√°n Path cho t·ª´ng class (Tr√™n kh√¥ng gian nh·ªè)
        paths_dict = {}
        unique_classes = np.unique(mask_small)
        for cls_id in unique_classes:
            if cls_id == 0: continue
            
            # T·∫°o binary mask cho class hi·ªán t·∫°i
            bin_mask = (mask_small == cls_id).astype(np.uint8) * 255
            
            # T√°ch l√†n v√† t√≠nh path ri√™ng bi·ªát
            paths = get_separated_paths(bin_mask, cls_id, scale_x, scale_y)
            paths_dict[cls_id] = paths
            
        # 4. V·∫Ω k·∫øt qu·∫£ l√™n Frame g·ªëc (High Quality Visualization)
        result = draw_results(frame, mask_small, paths_dict)
        
        # 5. FPS
        curr_time = time.time()
        fps = 1 / (curr_time - prev_time + 1e-6)
        prev_time = curr_time
        
        cv2.putText(result, f"FPS: {int(fps)}", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
        
        out.write(result)
        pbar.update(1)
        
    cap.release()
    out.release()
    pbar.close()
    print(f"\n‚úÖ Video saved: {save_path}")

def main():
    if not os.path.exists(MODEL_PATH):
        print("‚ùå Model path incorrect.")
        return
    
    predictor = LanePredictor(MODEL_PATH)
    process_video(predictor, INPUT_PATH)

if __name__ == "__main__":
    main()